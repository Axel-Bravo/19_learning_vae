import tensorflow as tf


class AE(tf.keras.Model):
    def __init__(self, input_shape, encoder_dim):
        """
        Convolutional Autoencoder
        """
        super(AE, self).__init__()
        self.encoder_dim = encoder_dim

        # Encoder
        self.input_enc = tf.keras.layers.InputLayer(input_shape=input_shape)
        self.conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')
        self.conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu')
        self.flatten = tf.keras.layers.Flatten()
        self.dense_1 = tf.keras.layers.Dense(self.encoder_dim)

        # Decoder
        self.input_dec = tf.keras.layers.InputLayer(input_shape=(self.encoder_dim,))

        self.dense_1_dec = tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu)
        self.reshape = tf.keras.layers.Reshape(target_shape=(7, 7, 32))
        self.conv_tp_1 = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding="SAME",
                                            activation='relu')
        self.conv_tp_2 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding="SAME",
                                            activation='relu')
        self.conv_tp_3 = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding="SAME")

    def encode(self, x):
        x = self.conv_1(self.input_enc(x))
        x = self.conv_2(x)
        x = self.flatten(x)
        x = self.dense_1(x)
        return x

    def decode(self, x):
        x = self.dense_1_dec(self.input_dec(x))
        x = self.reshape(x)
        x = self.conv_tp_1(x)
        x = self.conv_tp_2(x)
        x = self.conv_tp_3(x)
        return x

    def call(self, x):
        x = self.encode(x)
        x = self.decode(x)
        return x


class CVAE(tf.keras.Model):
    def __init__(self, latent_dim):
        """
        Convolutional Variational Autoencoder
        Based on the tutorial from: https://www.tensorflow.org/alpha/tutorials/generative/cvae
        :param latent_dim: latent dimension
        """
        super(CVAE, self).__init__()
        self.latent_dim = latent_dim
        self.inference_net = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'),
            tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'),
            tf.keras.layers.Flatten(),
            # No activation
            tf.keras.layers.Dense(latent_dim + latent_dim)])

        self.generative_net = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
            tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu),
            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),
            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2),
                                            padding="SAME", activation='relu'),
            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2),
                                            padding="SAME", activation='relu'),
            # No activation
            tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1),
                                            padding="SAME")])

    def sample(self, eps=None):
        """
        Generate sample for the generative net
        :param eps: epsilon
        :return: sample batch
        """
        if eps is None:
            eps = tf.random.normal(shape=(100, self.latent_dim))
        return self.decode(eps, apply_sigmoid=True)

    def encode(self, x):
        """
        Obtains the mena and log of the variance from the inference network
        :param x: sample(s) to be introduced into the inference network
        :return: mean and log variance from the inference network
        """
        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)
        return mean, logvar

    @staticmethod
    def reparameterize(mean, logvar):
        """
        Reparametrization trick, resamples from the distribution generated by the latent inference network
        :param mean: mean value
        :param logvar: log variance value
        :return: sampled value from the distribution
        """
        eps = tf.random.normal(shape=mean.shape)
        return eps * tf.exp(logvar * .5) + mean

    def decode(self, z, apply_sigmoid=False):
        """
        Generates an input from the re-sampled input distribution
        :param z: input from the latent distribution
        :param apply_sigmoid: either true/false
        :return: logits
        """
        logits = self.generative_net(z)
        if apply_sigmoid:
            probs = tf.sigmoid(logits)
            return probs
        return logits
